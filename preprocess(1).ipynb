{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = './data/ai_challenger_oqmrc_trainingset.json' # train set\n",
    "valid_path = './data/ai_challenger_oqmrc_validationset.json' # validation set\n",
    "test_path = './data/ai_challenger_oqmrc_testa.json' # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load my own dictionary from sougou to help jieba cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.load_userdict('./my_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_json(train_path, orient='records', encoding='utf-8', lines=True)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternatives</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>有|没有|无法确定</td>\n",
       "      <td>有</td>\n",
       "      <td>动漫好看的H：爱的魔法，KEY的作品，喧嚣学院，草莓100%，双恋，爱丽丝学园，灼眼的夏娜，...</td>\n",
       "      <td>有没有好看的h</td>\n",
       "      <td>250001</td>\n",
       "      <td>http://iask.sina.com.cn/key/5a18d46b84aedabb5c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>能</td>\n",
       "      <td>醋泡鸡蛋确实具有一定美白嫩化肌肤、提高皮肤亮度、祛斑的效果，因为白醋中含有的醋酸可以加速表皮...</td>\n",
       "      <td>醋泡鸡蛋真能去斑吗</td>\n",
       "      <td>250002</td>\n",
       "      <td>http://www.120ask.com/question/65970789.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>听不懂|听得懂|无法确定</td>\n",
       "      <td>听不懂</td>\n",
       "      <td>人有人言，兽有兽语。动物是不会听懂人说话的</td>\n",
       "      <td>老鼠听得懂人话吗</td>\n",
       "      <td>250003</td>\n",
       "      <td>http://wenwen.sogou.com/z/q166740184.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>无法确定|大|不大</td>\n",
       "      <td>无法确定</td>\n",
       "      <td>1.前期投资约5-10万元设备投资：柜台、门面装修、电脑及简单家具，一次性投入约2万元。2....</td>\n",
       "      <td>开洗车店投资大吗</td>\n",
       "      <td>250004</td>\n",
       "      <td>http://wenwen.sogou.com/z/q705319471.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>会|不会|无法确定</td>\n",
       "      <td>会</td>\n",
       "      <td>性接触没有保护措施，是有感染的几率的，艾滋病没有特异性的症状。</td>\n",
       "      <td>类似性行为会不会感染艾滋病</td>\n",
       "      <td>250005</td>\n",
       "      <td>http://www.169kang.com/question/166710467.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alternatives answer                                            passage  \\\n",
       "0     有|没有|无法确定      有  动漫好看的H：爱的魔法，KEY的作品，喧嚣学院，草莓100%，双恋，爱丽丝学园，灼眼的夏娜，...   \n",
       "1     能|不能|无法确定      能  醋泡鸡蛋确实具有一定美白嫩化肌肤、提高皮肤亮度、祛斑的效果，因为白醋中含有的醋酸可以加速表皮...   \n",
       "2  听不懂|听得懂|无法确定    听不懂                              人有人言，兽有兽语。动物是不会听懂人说话的   \n",
       "3     无法确定|大|不大   无法确定  1.前期投资约5-10万元设备投资：柜台、门面装修、电脑及简单家具，一次性投入约2万元。2....   \n",
       "4     会|不会|无法确定      会                    性接触没有保护措施，是有感染的几率的，艾滋病没有特异性的症状。   \n",
       "\n",
       "           query  query_id                                                url  \n",
       "0        有没有好看的h    250001  http://iask.sina.com.cn/key/5a18d46b84aedabb5c...  \n",
       "1      醋泡鸡蛋真能去斑吗    250002        http://www.120ask.com/question/65970789.htm  \n",
       "2       老鼠听得懂人话吗    250003           http://wenwen.sogou.com/z/q166740184.htm  \n",
       "3       开洗车店投资大吗    250004           http://wenwen.sogou.com/z/q705319471.htm  \n",
       "4  类似性行为会不会感染艾滋病    250005     http://www.169kang.com/question/166710467.html  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = pd.read_json(valid_path, orient='records', encoding='utf-8', lines=True)\n",
    "valid_set.head()\n",
    "#valid_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternatives</th>\n",
       "      <th>passage</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>武威公交一体化纪实 10家运输公司中标经营包括凉州区、古浪、民勤、天祝在内的城乡公交线路。经...</td>\n",
       "      <td>武威的公交卡古浪能不能用</td>\n",
       "      <td>280001</td>\n",
       "      <td>http://gsrb.gansudaily.com.cn/system/2009/08/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>现在这个社会什么买不到，只要你有钱是不是 欢迎光临【深圳平安安防】无线的有线的都有呢，看你喜...</td>\n",
       "      <td>能买到无线偷拍器吗</td>\n",
       "      <td>280002</td>\n",
       "      <td>http://wenwen.sogou.com/z/q701006723.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>是真的|不是真的|无法确定</td>\n",
       "      <td>请问朋友们网上中安信业代款是真的吗？ 【百度反诈骗联盟团队】特别提醒：网上发布的所有只凭身份...</td>\n",
       "      <td>中安信业减免还款是真实的吗</td>\n",
       "      <td>280003</td>\n",
       "      <td>http://wenwen.sogou.com/z/q763575352.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>对于这些的话也可以咨询一下你的直属上司或者是领导，他们专业的意见也都是可以的。</td>\n",
       "      <td>petct医保报销吗</td>\n",
       "      <td>280004</td>\n",
       "      <td>http://www.mama.cn/ask/q13547252-p1.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>慢热|不慢热|无法确定</td>\n",
       "      <td>在巨蟹座当中，慢热型的性格，更是让她们的爱心与细腻，更好的发挥到极致。</td>\n",
       "      <td>巨蟹座慢热么</td>\n",
       "      <td>280005</td>\n",
       "      <td>http://www.d1xz.net/astro/Cancer/art117849.aspx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alternatives                                            passage  \\\n",
       "0      能|不能|无法确定  武威公交一体化纪实 10家运输公司中标经营包括凉州区、古浪、民勤、天祝在内的城乡公交线路。经...   \n",
       "1      能|不能|无法确定  现在这个社会什么买不到，只要你有钱是不是 欢迎光临【深圳平安安防】无线的有线的都有呢，看你喜...   \n",
       "2  是真的|不是真的|无法确定  请问朋友们网上中安信业代款是真的吗？ 【百度反诈骗联盟团队】特别提醒：网上发布的所有只凭身份...   \n",
       "3      能|不能|无法确定            对于这些的话也可以咨询一下你的直属上司或者是领导，他们专业的意见也都是可以的。   \n",
       "4    慢热|不慢热|无法确定                在巨蟹座当中，慢热型的性格，更是让她们的爱心与细腻，更好的发挥到极致。   \n",
       "\n",
       "           query  query_id                                                url  \n",
       "0   武威的公交卡古浪能不能用    280001  http://gsrb.gansudaily.com.cn/system/2009/08/2...  \n",
       "1      能买到无线偷拍器吗    280002           http://wenwen.sogou.com/z/q701006723.htm  \n",
       "2  中安信业减免还款是真实的吗    280003           http://wenwen.sogou.com/z/q763575352.htm  \n",
       "3     petct医保报销吗    280004           http://www.mama.cn/ask/q13547252-p1.html  \n",
       "4         巨蟹座慢热么    280005    http://www.d1xz.net/astro/Cancer/art117849.aspx  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(test_path, orient='records', encoding='utf-8', lines=True)\n",
    "print (test_set.shape)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.851 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1', '=', '2', '/', '2', '=', '3', '/', '3', '=', '4', '/', '4', '=', '5', '/', '5', '=', 'n', '/', 'n'], ['x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x'])\n",
      "['醋', '泡', '鸡蛋', '确实', '具有', '一定', '美', '白嫩', '化', '肌肤', '、', '提高', '皮肤', '亮度', '、', '祛斑', '的', '效果']\n"
     ]
    }
   ],
   "source": [
    "def nlp_seg(text):\n",
    "    #NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "    #seg = NLPTokenizer.segment(text)\n",
    "    seg = pseg.cut(text)\n",
    "    words = []\n",
    "    tag = []\n",
    "    for w,f in seg:\n",
    "        words.append(w)\n",
    "        tag.append(f)\n",
    "    return words, tag\n",
    "print (nlp_seg('1=2/2=3/3=4/4=5/5=n/n'))\n",
    "print (jieba.lcut('醋泡鸡蛋确实具有一定美白嫩化肌肤、提高皮肤亮度、祛斑的效果'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, ['m', 'q', 'm', 'm', 'q', 'm', 'm', 'q', 'm', 'm', 'q', 'm', 'm', 'q', 'nx', 'nx'])\n"
     ]
    }
   ],
   "source": [
    "# nshort segment is more precise for NER\n",
    "def pos_seg(text):\n",
    "    '''\n",
    "    NShortSegment = JClass(\"com.hankcs.hanlp.seg.NShort.NShortSegment\")\n",
    "    segment = NShortSegment().enableCustomDictionary(False).enablePlaceRecognize(True).enableOrganizationRecognize(True)\n",
    "    seg = segment.seg(text)\n",
    "    '''\n",
    "    NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "    seg = []\n",
    "    for t in text.split():\n",
    "        seg.extend(NLPTokenizer.segment(t))\n",
    "    \n",
    "    w = []\n",
    "    l = []\n",
    "    for i in seg:\n",
    "        if str(i.nature)[0] != 'w':\n",
    "            w.append(i.word)\n",
    "            l.append(str(i.nature))\n",
    "    tag_fea = []\n",
    "    for t in l:\n",
    "        tag = [0] * 5\n",
    "        if t[0] == 'a' or t[0] == 'd': # 形容词，副词\n",
    "            tag[0] = 1\n",
    "        elif t[0] == 'm' or t[0] == 'q' or t[0] == 't': # 数词，量词，时间词\n",
    "            tag[1] = 1\n",
    "        elif t[0] == 'n' or t[0] == 'r' or t[0] == 's' or t[0] == 'g': # 名词，代词，处所词, 学术词汇\n",
    "            tag[2] = 1\n",
    "        elif t[0] == 'c' or t[0] == 'p' or t[0] == 'u': # 连词，介词\n",
    "            tag[3] = 1\n",
    "        elif t[0] == 'v': # 动词\n",
    "            tag[4] = 1\n",
    "        tag_fea.append(tag)\n",
    "    return len(w), len(' '.join(w).split(' ')), l\n",
    "print (pos_seg('1=2/2=3/3=4/4=5/5=n/n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess function to clean a text \n",
    "cut words, remove punctuation, lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text, alternatives, aug=False):\n",
    "    sent, tag = pos_seg(text)\n",
    "\n",
    "    for i in range(len(sent)):\n",
    "        if aug and random.random()<0.1: # data augmentation\n",
    "            sent[i] = ' '\n",
    "            del(tag[i])\n",
    "        else:\n",
    "            sent[i].lower()\n",
    "    sent = ' '.join(sent)\n",
    "    return sent, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the query and alternatives\n",
    "Use the preprocessed query, remove the last 吗 or 么. If same word exists, replace it with the current option. Otherwise put the option in the head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate query and alternatives\n",
    "def query_alt(query, alternatives, a):\n",
    "    '''\n",
    "    query: line['query'] from original dataframe\n",
    "    alternatives: line['alternatives'] from original dataframe\n",
    "    a: current option in alternatives to be merged with query\n",
    "    \n",
    "    return: query and current option a concatenated (preprocessed)\n",
    "    '''\n",
    "    \n",
    "    query = query.strip()\n",
    "    if query[-1] == \"吗\" or query[-1] == \"么\" or query[-1] == \"嘛\" or query[-1] == \"不\": \n",
    "        query = query[:-1]\n",
    "        match = None\n",
    "        o = alternatives.split('|')\n",
    "        o = [m.strip() for m in o]\n",
    "        if '无法确认' in o:\n",
    "            o.remove('无法确认')\n",
    "        if '无法确定' in o:\n",
    "            o.remove('无法确定') \n",
    "        if o[0] in o[1]:\n",
    "            long = o[1]\n",
    "            short = o[0]\n",
    "        else:\n",
    "            long = o[0]\n",
    "            short = o[1]\n",
    "        if long in query:\n",
    "            match = long\n",
    "        else:\n",
    "            if short in query:\n",
    "                match = short\n",
    "            elif (short == '能') and ('可以' in query):\n",
    "                match = '可以'\n",
    "            elif (short == '可以') and ('能' in query):\n",
    "                match = '能'\n",
    "            elif (short == '可以') and ('会' in query):\n",
    "                match = '会'\n",
    "            elif (short == '会') and ('可以' in query):\n",
    "                match = '可以'\n",
    "            elif (short == '会') and ('能' in query):\n",
    "                match = '能'\n",
    "            elif (short == '能') and ('会' in query):\n",
    "                match = '会'\n",
    "\n",
    "        if match:\n",
    "            query = query.replace(match, a)\n",
    "        else:\n",
    "            query = a + query\n",
    "            \n",
    "        merged = preprocess(query, alternatives)\n",
    "        return merged\n",
    "            \n",
    "    else: # 问题里正反两个词都要替换\n",
    "        match = alternatives.split('|')\n",
    "        match = [m.strip() for m in match]\n",
    "        if '无法确认' in match:\n",
    "            match.remove('无法确认')\n",
    "        if '无法确定' in match:\n",
    "            match.remove('无法确定') \n",
    "        if match[0] in query and match[1] in query: # 两个词都出现了\n",
    "            if match[0] + match[1] in query: # 有没有，会不会\n",
    "                query = query.replace(match[0] + match[1], a)\n",
    "            elif match[1] + match[0] in query:\n",
    "                query = query.replace(match[1] + match[0], a)\n",
    "            else: # A好还是B好\n",
    "                if a == match[0]:\n",
    "                    query = query.replace(match[1], ' ')\n",
    "                elif a == match[1]:\n",
    "                    query = query.replace(match[0], ' ')\n",
    "                else: # 无法确定\n",
    "                    query = query.replace(match[0], ' ')\n",
    "                    query = query.replace(match[1], a)\n",
    "        else: # 两个词没完整出现\n",
    "            if '能否' in query:\n",
    "                query = query.replace('能否', a)\n",
    "            elif '是否' in query:\n",
    "                query = query.replace('是否', a)\n",
    "            elif '可否' in query:\n",
    "                query = query.replace('可否', a)\n",
    "            \n",
    "        merged = preprocess(query, alternatives)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write into the tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 250000/250000 [28:04<00:00, 163.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pos_p = []\n",
    "train_pos_q = []\n",
    "with open('./data/train1.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t' + 'option' + '\\t' + 'label' + '\\n')\n",
    "    for i in tqdm(range(train_set.shape[0])):\n",
    "        line = train_set.iloc[i]\n",
    "        p, p_tag = preprocess(line['passage'], line['alternatives'])\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            a = a.strip()\n",
    "            m, m_tag = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            train_pos_p.append(p_tag)\n",
    "            train_pos_q.append(m_tag)\n",
    "            if a == line['answer'].strip():\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ p+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '1'+'\\n')\n",
    "            else:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ p+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '0'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [03:23<00:00, 147.76it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_pos_p = []\n",
    "valid_pos_q = []\n",
    "with open('./data/valid1.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t' + 'option' + '\\t' + 'label' + '\\n')\n",
    "    for i in tqdm(range(valid_set.shape[0])):\n",
    "        line = valid_set.iloc[i]\n",
    "        p, p_tag = preprocess(line['passage'], line['alternatives'])\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            a = a.strip()\n",
    "            m, m_tag = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            valid_pos_p.append(p_tag)\n",
    "            valid_pos_q.append(m_tag)\n",
    "            if a == line['answer'].strip():\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ p+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '1'+'\\n')\n",
    "            else:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ p+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '0'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [01:07<00:00, 147.67it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pos_p = []\n",
    "test_pos_q = []\n",
    "with open('./data/test1.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t' + 'option' + '\\n')\n",
    "    for i in tqdm(range(test_set.shape[0])):\n",
    "        line = test_set.iloc[i]\n",
    "        p, p_tag = preprocess(line['passage'], line['alternatives'])\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            a = a.strip()\n",
    "            m, m_tag = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            test_pos_p.append(p_tag)\n",
    "            test_pos_q.append(m_tag)\n",
    "            fw.write(str(line['query_id'])+ '\\t'+ p+ '\\t'+ m+ '\\t'+ a+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color=red>测试集test A 有两条有误！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "289730只有一个选项，无法确定。\n",
    "\n",
    "289334只有两个选项，不能or无法确定。选不能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part II: Add features for words in passage\n",
    "Add Exact match, soft align and Pos tag features. Refer to SAN model: https://arxiv.org/abs/1712.03556\n",
    "\n",
    "Delete soft-align and pos tag. Add option exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen_p = 150\n",
    "maxlen_q = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = './data/train1.tsv' # train set\n",
    "valid_path = './data/valid1.tsv' # validation set\n",
    "test_path = './data/test1.tsv' # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path, sep='\\t', header=0)\n",
    "valid = pd.read_csv(valid_path, sep='\\t', header=0)\n",
    "test = pd.read_csv(test_path, sep='\\t', header=0)\n",
    "#print (train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "print (len(train.iloc[1980]['passage'].split(' ')))\n",
    "print (len(train_pos_p[1980]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/750000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b3293c83199b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mfea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mom\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mfea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mpfea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfea\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    line = train.iloc[i]\n",
    "    q_words = line['query'].split(' ')\n",
    "    p_words = line['passage'].split(' ')\n",
    "    option = line['option']\n",
    "    pos_p = train_pos_p[i]\n",
    "    pos_q = train_pos_q[i]\n",
    "    #print (len(pos_p)==len(p_words), i)\n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "        pos_p = pos_p[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "        pos_q = pos_q[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for i, w in enumerate(p_words):\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_p[i])\n",
    "        pfea.append(fea)\n",
    "        \n",
    "    qfea = []\n",
    "    for i, w in enumerate(q_words):\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_q[i])\n",
    "        qfea.append(fea)\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 8)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 8)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/train_fea_p3', pl)\n",
    "np.save('./data/train_fea_q3', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 90000/90000 [00:25<00:00, 3520.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 150, 8) (90000, 15, 8)\n"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(valid.shape[0])):\n",
    "    line = valid.iloc[i]\n",
    "    q_words = line['query'].split(' ')\n",
    "    p_words = line['passage'].split(' ')\n",
    "    option = line['option']\n",
    "    pos_p = valid_pos_p[i]\n",
    "    pos_q = valid_pos_q[i]\n",
    "    \n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "        pos_p = pos_p[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "        pos_q = pos_q[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for i, w in enumerate(p_words):\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_p[i])\n",
    "        pfea.append(fea)\n",
    "        \n",
    "    qfea = []\n",
    "    for i, w in enumerate(q_words):\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_q[i])\n",
    "        qfea.append(fea)\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 8)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 8)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/valid_fea_p3', pl)\n",
    "np.save('./data/valid_fea_q3', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:08<00:00, 3599.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 150, 8) (30000, 15, 8)\n"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    line = test.iloc[i]\n",
    "    q_words = line['query'].split(' ')\n",
    "    p_words = line['passage'].split(' ')\n",
    "    option = line['option']\n",
    "    pos_p = test_pos_p[i]\n",
    "    pos_q = test_pos_q[i]\n",
    "    \n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "        pos_p = pos_p[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "        pos_q = pos_q[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for i, w in enumerate(p_words):\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_p[i])\n",
    "        pfea.append(fea)\n",
    "        \n",
    "    qfea = []\n",
    "    for i, w in enumerate(q_words):\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        fea = [em, om]\n",
    "        fea.extend(pos_q[i])\n",
    "        qfea.append(fea)\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 8)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 8)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/test_fea_p3', pl)\n",
    "np.save('./data/test_fea_q3', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
