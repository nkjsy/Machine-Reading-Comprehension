{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/ai_challenger_oqmrc_trainingset.json' # train set\n",
    "valid_path = './data/ai_challenger_oqmrc_validationset.json' # validation set\n",
    "test_path = './data/ai_challenger_oqmrc_testa.json' # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load my own dictionary from sougou to help jieba cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.load_userdict('./my_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_json(train_path, orient='records', encoding='utf-8', lines=True)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternatives</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>有|没有|无法确定</td>\n",
       "      <td>有</td>\n",
       "      <td>动漫好看的H：爱的魔法，KEY的作品，喧嚣学院，草莓100%，双恋，爱丽丝学园，灼眼的夏娜，...</td>\n",
       "      <td>有没有好看的h</td>\n",
       "      <td>250001</td>\n",
       "      <td>http://iask.sina.com.cn/key/5a18d46b84aedabb5c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>能</td>\n",
       "      <td>醋泡鸡蛋确实具有一定美白嫩化肌肤、提高皮肤亮度、祛斑的效果，因为白醋中含有的醋酸可以加速表皮...</td>\n",
       "      <td>醋泡鸡蛋真能去斑吗</td>\n",
       "      <td>250002</td>\n",
       "      <td>http://www.120ask.com/question/65970789.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>听不懂|听得懂|无法确定</td>\n",
       "      <td>听不懂</td>\n",
       "      <td>人有人言，兽有兽语。动物是不会听懂人说话的</td>\n",
       "      <td>老鼠听得懂人话吗</td>\n",
       "      <td>250003</td>\n",
       "      <td>http://wenwen.sogou.com/z/q166740184.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>无法确定|大|不大</td>\n",
       "      <td>无法确定</td>\n",
       "      <td>1.前期投资约5-10万元设备投资：柜台、门面装修、电脑及简单家具，一次性投入约2万元。2....</td>\n",
       "      <td>开洗车店投资大吗</td>\n",
       "      <td>250004</td>\n",
       "      <td>http://wenwen.sogou.com/z/q705319471.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>会|不会|无法确定</td>\n",
       "      <td>会</td>\n",
       "      <td>性接触没有保护措施，是有感染的几率的，艾滋病没有特异性的症状。</td>\n",
       "      <td>类似性行为会不会感染艾滋病</td>\n",
       "      <td>250005</td>\n",
       "      <td>http://www.169kang.com/question/166710467.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alternatives answer                                            passage  \\\n",
       "0     有|没有|无法确定      有  动漫好看的H：爱的魔法，KEY的作品，喧嚣学院，草莓100%，双恋，爱丽丝学园，灼眼的夏娜，...   \n",
       "1     能|不能|无法确定      能  醋泡鸡蛋确实具有一定美白嫩化肌肤、提高皮肤亮度、祛斑的效果，因为白醋中含有的醋酸可以加速表皮...   \n",
       "2  听不懂|听得懂|无法确定    听不懂                              人有人言，兽有兽语。动物是不会听懂人说话的   \n",
       "3     无法确定|大|不大   无法确定  1.前期投资约5-10万元设备投资：柜台、门面装修、电脑及简单家具，一次性投入约2万元。2....   \n",
       "4     会|不会|无法确定      会                    性接触没有保护措施，是有感染的几率的，艾滋病没有特异性的症状。   \n",
       "\n",
       "           query  query_id                                                url  \n",
       "0        有没有好看的h    250001  http://iask.sina.com.cn/key/5a18d46b84aedabb5c...  \n",
       "1      醋泡鸡蛋真能去斑吗    250002        http://www.120ask.com/question/65970789.htm  \n",
       "2       老鼠听得懂人话吗    250003           http://wenwen.sogou.com/z/q166740184.htm  \n",
       "3       开洗车店投资大吗    250004           http://wenwen.sogou.com/z/q705319471.htm  \n",
       "4  类似性行为会不会感染艾滋病    250005     http://www.169kang.com/question/166710467.html  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = pd.read_json(valid_path, orient='records', encoding='utf-8', lines=True)\n",
    "valid_set.head()\n",
    "#valid_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternatives</th>\n",
       "      <th>passage</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>武威公交一体化纪实 10家运输公司中标经营包括凉州区、古浪、民勤、天祝在内的城乡公交线路。经...</td>\n",
       "      <td>武威的公交卡古浪能不能用</td>\n",
       "      <td>280001</td>\n",
       "      <td>http://gsrb.gansudaily.com.cn/system/2009/08/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>现在这个社会什么买不到，只要你有钱是不是 欢迎光临【深圳平安安防】无线的有线的都有呢，看你喜...</td>\n",
       "      <td>能买到无线偷拍器吗</td>\n",
       "      <td>280002</td>\n",
       "      <td>http://wenwen.sogou.com/z/q701006723.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>是真的|不是真的|无法确定</td>\n",
       "      <td>请问朋友们网上中安信业代款是真的吗？ 【百度反诈骗联盟团队】特别提醒：网上发布的所有只凭身份...</td>\n",
       "      <td>中安信业减免还款是真实的吗</td>\n",
       "      <td>280003</td>\n",
       "      <td>http://wenwen.sogou.com/z/q763575352.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>能|不能|无法确定</td>\n",
       "      <td>对于这些的话也可以咨询一下你的直属上司或者是领导，他们专业的意见也都是可以的。</td>\n",
       "      <td>petct医保报销吗</td>\n",
       "      <td>280004</td>\n",
       "      <td>http://www.mama.cn/ask/q13547252-p1.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>慢热|不慢热|无法确定</td>\n",
       "      <td>在巨蟹座当中，慢热型的性格，更是让她们的爱心与细腻，更好的发挥到极致。</td>\n",
       "      <td>巨蟹座慢热么</td>\n",
       "      <td>280005</td>\n",
       "      <td>http://www.d1xz.net/astro/Cancer/art117849.aspx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alternatives                                            passage  \\\n",
       "0      能|不能|无法确定  武威公交一体化纪实 10家运输公司中标经营包括凉州区、古浪、民勤、天祝在内的城乡公交线路。经...   \n",
       "1      能|不能|无法确定  现在这个社会什么买不到，只要你有钱是不是 欢迎光临【深圳平安安防】无线的有线的都有呢，看你喜...   \n",
       "2  是真的|不是真的|无法确定  请问朋友们网上中安信业代款是真的吗？ 【百度反诈骗联盟团队】特别提醒：网上发布的所有只凭身份...   \n",
       "3      能|不能|无法确定            对于这些的话也可以咨询一下你的直属上司或者是领导，他们专业的意见也都是可以的。   \n",
       "4    慢热|不慢热|无法确定                在巨蟹座当中，慢热型的性格，更是让她们的爱心与细腻，更好的发挥到极致。   \n",
       "\n",
       "           query  query_id                                                url  \n",
       "0   武威的公交卡古浪能不能用    280001  http://gsrb.gansudaily.com.cn/system/2009/08/2...  \n",
       "1      能买到无线偷拍器吗    280002           http://wenwen.sogou.com/z/q701006723.htm  \n",
       "2  中安信业减免还款是真实的吗    280003           http://wenwen.sogou.com/z/q763575352.htm  \n",
       "3     petct医保报销吗    280004           http://www.mama.cn/ask/q13547252-p1.html  \n",
       "4         巨蟹座慢热么    280005    http://www.d1xz.net/astro/Cancer/art117849.aspx  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_json(test_path, orient='records', encoding='utf-8', lines=True)\n",
    "print (test_set.shape)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aa01A02= 人类 生人 全人类\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open('./tongyici/tongyici.txt','r', encoding='gbk')\n",
    "lines=f.readlines()\n",
    "print (lines[1])\n",
    "sym_words=[]\n",
    "for line in lines:\n",
    "    line=line.replace('\\n','')\n",
    "    items=line.split(' ')\n",
    "    index=items[0]\n",
    "    if(index[-1]=='='):\n",
    "        sym_words.append(items[1:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(l):\n",
    "    for w in l:\n",
    "        for each in sym_words:\n",
    "            if w in each:\n",
    "                w = each\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess function to clean a text \n",
    "cut words, remove punctuation, lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, alternatives, aug=False):\n",
    "    '''\n",
    "    dic = {}\n",
    "    for i in alternatives.split('|'):\n",
    "        if not (i == '无法确定' or i == '无法确认'):\n",
    "            f = jieba.get_FREQ(i, 0)\n",
    "            #print (f)\n",
    "            dic[i] = f\n",
    "            jieba.add_word(i)\n",
    "    '''\n",
    "    sent = jieba.lcut(text, HMM=False)\n",
    "    #sent = nlp_seg(text)\n",
    "    '''\n",
    "    for i in dic:\n",
    "        jieba.del_word(i)\n",
    "        jieba.add_word(i, dic[i])\n",
    "    '''\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] in \"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）：【】]+\":\n",
    "            sent[i] = ' '\n",
    "        elif aug and random.random()<0.1: # data augmentation\n",
    "            sent[i] = ' '\n",
    "        else:\n",
    "            sent[i].lower()\n",
    "    sent = expand(sent)\n",
    "    sent = ' '.join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the query and alternatives\n",
    "Use the preprocessed query, remove the last 吗 or 么. If same word exists, replace it with the current option. Otherwise put the option in the head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate query and alternatives\n",
    "def query_alt(query, alternatives, a):\n",
    "    '''\n",
    "    query: line['query'] from original dataframe\n",
    "    alternatives: line['alternatives'] from original dataframe\n",
    "    a: current option in alternatives to be merged with query\n",
    "    \n",
    "    return: query and current option a concatenated (preprocessed)\n",
    "    '''\n",
    "    \n",
    "    query = preprocess(query, alternatives).split()\n",
    "    if query[-1] == \"吗\" or query[-1] == \"么\" or query[-1] == \"嘛\" or query[-1] == \"不\":\n",
    "        del query[-1]\n",
    "    '''\n",
    "    match = None # 问题里要替换的词\n",
    "    for i in alternatives.split('|'):\n",
    "        if i in query:\n",
    "            match = i\n",
    "            break\n",
    "        elif (i == '能') and ('可以' in query):\n",
    "            match = '可以'\n",
    "            break\n",
    "        elif (i == '可以') and ('能' in query):\n",
    "            match = '能'\n",
    "            break\n",
    "        elif (i == '可以') and ('会' in query):\n",
    "            match = '会'\n",
    "            break\n",
    "        elif (i == '会') and ('可以' in query):\n",
    "            match = '可以'\n",
    "            break\n",
    "        elif (i == '会') and ('能' in query):\n",
    "            match = '能'\n",
    "            break\n",
    "        elif (i == '能') and ('会' in query):\n",
    "            match = '会'\n",
    "            break\n",
    "            \n",
    "    if match:\n",
    "        ind = query.index(match)\n",
    "        query[ind] = a\n",
    "        merged = ' '.join(query)\n",
    "    else:\n",
    "        l = [a]\n",
    "        l.extend(query)\n",
    "        merged = ' '.join(l)\n",
    "    '''\n",
    "    l = [a]\n",
    "    l.extend(query)\n",
    "    merged = ' '.join(l)\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write into the tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/250000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SJ\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.587 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "  0%|▏                                                                         | 430/250000 [01:39<10:47:24,  6.42it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-41c3c08727d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malternatives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alternatives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'answer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'passage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alternatives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'passage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alternatives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'0'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-233e0ef44ab2>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text, alternatives, aug)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-d0340731826d>\u001b[0m in \u001b[0;36mexpand\u001b[1;34m(l)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msym_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meach\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meach\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('.//data//train.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t' + 'option' + '\\t' + 'label' + '\\n')\n",
    "    for i in tqdm(range(train_set.shape[0])):\n",
    "        line = train_set.iloc[i]\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            m = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            if a == line['answer']:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ preprocess(line['passage'], line['alternatives'], True)+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '1'+'\\n')\n",
    "            else:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ preprocess(line['passage'], line['alternatives'], True)+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '0'+'\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [00:40<00:00, 733.97it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/valid.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t' + 'option' + '\\t' + 'label' + '\\n')\n",
    "    for i in tqdm(range(valid_set.shape[0])):\n",
    "        line = valid_set.iloc[i]\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            m = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            if a == line['answer']:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ preprocess(line['passage'], line['alternatives'])+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '1'+'\\n')\n",
    "            else:\n",
    "                fw.write(str(line['query_id'])+ '\\t'+ preprocess(line['passage'], line['alternatives'])+ '\\t'+ m+ '\\t'+ a+ '\\t'+ '0'+'\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 753.40it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/test.tsv', 'w', encoding='utf-8') as fw:\n",
    "    fw.write('id' + '\\t' + 'passage' + '\\t' + 'query' + '\\t'+ 'option'+ '\\n')\n",
    "    for i in tqdm(range(test_set.shape[0])):\n",
    "        line = test_set.iloc[i]\n",
    "        for a in line['alternatives'].split('|'):\n",
    "            m = query_alt(query=line['query'], alternatives=line['alternatives'], a=a)\n",
    "            fw.write(str(line['query_id'])+ '\\t'+ preprocess(line['passage'], line['alternatives'])+ '\\t'+ m+ '\\t'+ a+ '\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color=red>测试集test A 有两条有误！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "289730只有一个选项，无法确定。\n",
    "\n",
    "289334只有两个选项，不能or无法确定。选不能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part II: Add features for words in passage\n",
    "Add Exact match, soft align and Pos tag features. Refer to SAN model: https://arxiv.org/abs/1712.03556\n",
    "\n",
    "Delete soft-align and pos tag. Add option exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_p = 150\n",
    "maxlen_q = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.tsv' # train set\n",
    "valid_path = './data/valid.tsv' # validation set\n",
    "test_path = './data/test.tsv' # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 5) (90000, 5) (29997, 4)\n",
      "       id                                            passage  \\\n",
      "0  250001  动漫 好看 的 H   爱 的 魔法   KEY 的 作品   喧嚣 学院   草莓 100...   \n",
      "1  250001  动漫 好看 的 H   爱 的 魔法   KEY 的 作品   喧嚣 学院   草莓 100...   \n",
      "2  250001  动漫 好看 的 H   爱 的 魔法   KEY 的 作品   喧嚣 学院   草莓 100...   \n",
      "3  250002  醋 泡 鸡蛋 确实 具有 一定 美 白嫩 化 肌肤   提高 皮肤 亮度   祛斑 的 效果...   \n",
      "4  250002  醋 泡 鸡蛋 确实 具有 一定 美 白嫩 化 肌肤   提高 皮肤 亮度   祛斑 的 效果...   \n",
      "\n",
      "               query option  label  \n",
      "0       有 有没有 好看 的 h      有      1  \n",
      "1      没有 有没有 好看 的 h     没有      0  \n",
      "2    无法确定 有没有 好看 的 h   无法确定      0  \n",
      "3   能 醋 泡 鸡蛋 真 能 去 斑      能      1  \n",
      "4  不能 醋 泡 鸡蛋 真 能 去 斑     不能      0  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path, sep='\\t', header=0)\n",
    "valid = pd.read_csv(valid_path, sep='\\t', header=0)\n",
    "test = pd.read_csv(test_path, sep='\\t', header=0)\n",
    "print (train.shape, valid.shape, test.shape)\n",
    "print (valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 750000/750000 [03:19<00:00, 3756.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 150, 2) (750000, 15, 2)\n"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    line = train.iloc[i]\n",
    "    q_words = line['query'].split()\n",
    "    p_words = line['passage'].split()\n",
    "    option = line['option']\n",
    "    \n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for w in p_words:\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        pfea.append([em, om])\n",
    "        \n",
    "    qfea = []\n",
    "    for w in q_words:\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        qfea.append([em, om])\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 2)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 2)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/train_fea_p', pl)\n",
    "np.save('./data/train_fea_q', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 90000/90000 [00:22<00:00, 3982.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 150, 2) (90000, 15, 2)\n"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(valid.shape[0])):\n",
    "    line = valid.iloc[i]\n",
    "    q_words = line['query'].split()\n",
    "    p_words = line['passage'].split()\n",
    "    option = line['option']\n",
    "    \n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for w in p_words:\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        pfea.append([em, om])\n",
    "        \n",
    "    qfea = []\n",
    "    for w in q_words:\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        qfea.append([em, om])\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 2)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 2)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/valid_fea_p', pl)\n",
    "np.save('./data/valid_fea_q', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 29997/29997 [00:08<00:00, 3665.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29997, 150, 2) (29997, 15, 2)\n"
     ]
    }
   ],
   "source": [
    "pl = []\n",
    "ql = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    line = test.iloc[i]\n",
    "    q_words = line['query'].split()\n",
    "    p_words = line['passage'].split()\n",
    "    option = line['option']\n",
    "    \n",
    "    if len(p_words) > maxlen_p: # truncate pre\n",
    "        lt = len(p_words) - maxlen_p\n",
    "        p_words = p_words[lt:]\n",
    "    if len(q_words) > maxlen_q: # truncate post\n",
    "        q_words = q_words[:maxlen_q]\n",
    "\n",
    "    pfea = []\n",
    "    for w in p_words:\n",
    "        # exact match\n",
    "        if w in q_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        pfea.append([em, om])\n",
    "        \n",
    "    qfea = []\n",
    "    for w in q_words:\n",
    "        # exact match\n",
    "        if w in p_words:\n",
    "            em = 1\n",
    "        else:\n",
    "            em = 0\n",
    "        # option match\n",
    "        if w == option:\n",
    "            om = 1\n",
    "        else:\n",
    "            om = 0\n",
    "        qfea.append([em, om])\n",
    "        \n",
    "    while len(pfea) < maxlen_p: # pad with 0 pre\n",
    "        pfea.insert(0, [0] * 2)\n",
    "    pl.append(pfea)\n",
    "    while len(qfea) < maxlen_q: # pad with 0 post\n",
    "        qfea.append([0] * 2)\n",
    "    ql.append(qfea)\n",
    "\n",
    "pl = np.asarray(pl)\n",
    "ql = np.asarray(ql)\n",
    "np.save('./data/test_fea_p', pl)\n",
    "np.save('./data/test_fea_q', ql)\n",
    "print (np.shape(pl), np.shape(ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
